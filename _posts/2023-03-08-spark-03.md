---
layout: single
title: "[Spark] Spark Streaming (1)"
category: "BigData"
tag: [spark]
author_profile: false
sidebar:
  nav: "docs"
---

# Spark 이론 2 :
Spark 사용방법, Spark Streaming 처리 방식 

## 1. Spark 사용 방법

Spark 사용 방법 2가지를 먼저 알아 보자.

- 스파크 독립 애플리케이션
  - 스파크 라이브러리(스파크 API)를 사용해 독립형 애플리케이션 코드 작성
  - 스파크에 코드 제출해서 실행

- 스파크 셸(스파크 REPL)
  - 스칼라 셸 / 파이썬 셸
  - 명령 결과 반환한 이후에도 프롬프트 유지, 셸 종료 시 프로그램 삭제됨
  - 일회성 작업(EDA, 빠른 가설 검정 등)에 편리


## 2. Spark Streaming

Spark는 정형 및 비정형 데이터의 일괄 처리 작업에 적합하다. 하지만 미니배치(mini-batch)를 통해 실시간 데이터에도 연산을 적용할 수 있다.

### 2.1. 처리 방식

![spark-03-01](https://raw.githubusercontent.com/hanalog/hanalog.github.io/gh-pages/images/2023-03-08-spark-03/spark-03-01.png)

- 다음은 Spark Streaming의 데이터 처리 방식이다.
  1. 다양한 소스에서 데이터 입수
     - Spark Streaming은 각 데이터 소스별로 별도의 리시버 제공
  2. 리시버가 데이터 입수 후 Spark Streaming에 전달
  3. 리시버가 전달한 데이터를 Spark Streaming이 미니배치 RDD로 주기적 분할
  4. 스파크 API가 미니배치RDD를 일반 RDD처럼 처리
  5. 미니배치 처리 결과를 다른 시스템에 전달(HDFS, DB, Kafka 등)
- Spark Streaming은 반드시 코어(스레드)를 2개 이상 실행자에 할당해야 한다.
  - 리시버에게 최소 1개
    - 입력 데이터 스트림을 처리하기 위해 리시버 하나 당 코어 1개씩 사용
  - 프로그램 연산 수행을 위해 최소 1개

## 3. 생각

- 미니배치를 통해 처리하기 때문에 대량의 실시간 데이터를 처리하는 경우에는 지연시간으로 인해 적합하지 않을 것이다. 

  - 이 경우에는 네이티브 스트림을 지원하는 Flink가 더 좋다. Flink의 네이티브 스트림을 사용하기 위해서는 Java를 알아야 한다.

- 그럼에도 다양한 연산 기능을 지원하는 스파크 API를 실시간 데이터에 바로 적용해서 처리할 수 있다는 점은 큰 장점이다.

  

## REFERENCES

- [서적] 스파크를 다루는 기술 (페타 제체비치, 마르코 보나치)